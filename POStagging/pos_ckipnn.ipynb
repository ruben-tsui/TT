{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dIW4DlcP4Rpe"
   },
   "source": [
    "Do NOT execute the following unless absolutely necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a6LALBTfFlMv"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "!wget -c http://ckip.iis.sinica.edu.tw/data/ckiptagger/data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D9MxArho4auP"
   },
   "source": [
    "Mount Google Drive (for persistent data storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E3MMW1kutHID"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Name: tensorflow\n",
      "Version: 1.15.2\n",
      "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
      "Home-page: https://www.tensorflow.org/\n",
      "Author: Google Inc.\n",
      "Author-email: packages@tensorflow.org\n",
      "License: Apache 2.0\n",
      "Location: /srv/conda/envs/notebook/lib/python3.7/site-packages\n",
      "Requires: absl-py, tensorboard, protobuf, grpcio, keras-preprocessing, numpy, wheel, astor, gast, termcolor, wrapt, tensorflow-estimator, keras-applications, google-pasta, opt-einsum, six\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "# Check TensorFlow version to make sure it's 1.5.x (or else CKIPTagger won't run)\n",
    "!pip show tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6hBOPoLm4wXO"
   },
   "source": [
    "Install CKIP neural network-based word segmentation and POS-tagger modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fn1LlYESG5a6"
   },
   "outputs": [],
   "source": [
    "!python -m pip install -U ckiptagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aAfJ0O_05MQx"
   },
   "source": [
    "Now upnload your sample traditional Chinese text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zGwT9dOEwhLm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xIvwczsWfNXc"
   },
   "source": [
    "Import WS and POS models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ut7rX_5YfMM9"
   },
   "outputs": [],
   "source": [
    "from ckiptagger import data_utils, construct_dictionary, WS, POS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ny7jZJnF6N8_"
   },
   "source": [
    "Load word segmentation (WS) model and/or POS-tagging model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hEutD0aDHkAb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12 s, sys: 4.82 s, total: 16.8 s\n",
      "Wall time: 18.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ws = WS(\"./ckipnn_data\")\n",
    "pos = POS(\"./ckipnn_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xzkYz8wNHrZu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 lines processed...\n",
      "200 lines processed...\n",
      "300 lines processed...\n",
      "400 lines processed...\n",
      "500 lines processed...\n",
      "600 lines processed...\n",
      "700 lines processed...\n",
      "800 lines processed...\n",
      "900 lines processed...\n",
      "1000 lines processed...\n",
      "1100 lines processed...\n",
      "1200 lines processed...\n",
      "1300 lines processed...\n",
      "1400 lines processed...\n",
      "1500 lines processed...\n",
      "1600 lines processed...\n",
      "1700 lines processed...\n",
      "1800 lines processed...\n",
      "1900 lines processed...\n",
      "2000 lines processed...\n",
      "2100 lines processed...\n",
      "2200 lines processed...\n",
      "2300 lines processed...\n",
      "2400 lines processed...\n",
      "2500 lines processed...\n",
      "2600 lines processed...\n",
      "2700 lines processed...\n",
      "2800 lines processed...\n",
      "2900 lines processed...\n",
      "3000 lines processed...\n",
      "3100 lines processed...\n",
      "3200 lines processed...\n",
      "3300 lines processed...\n",
      "3400 lines processed...\n",
      "3500 lines processed...\n",
      "3600 lines processed...\n",
      "3700 lines processed...\n",
      "3800 lines processed...\n",
      "3900 lines processed...\n",
      "4000 lines processed...\n",
      "4100 lines processed...\n",
      "4200 lines processed...\n",
      "4300 lines processed...\n",
      "4400 lines processed...\n",
      "4500 lines processed...\n",
      "4600 lines processed...\n",
      "4700 lines processed...\n",
      "4800 lines processed...\n",
      "4900 lines processed...\n",
      "5000 lines processed...\n",
      "5100 lines processed...\n",
      "5200 lines processed...\n",
      "5300 lines processed...\n",
      "5400 lines processed...\n",
      "5500 lines processed...\n",
      "5600 lines processed...\n",
      "5700 lines processed...\n",
      "5800 lines processed...\n",
      "5900 lines processed...\n",
      "6000 lines processed...\n",
      "6100 lines processed...\n",
      "6200 lines processed...\n",
      "6300 lines processed...\n",
      "6400 lines processed...\n",
      "6500 lines processed...\n",
      "6600 lines processed...\n",
      "6700 lines processed...\n",
      "6800 lines processed...\n",
      "6900 lines processed...\n",
      "7000 lines processed...\n",
      "7100 lines processed...\n",
      "7200 lines processed...\n",
      "7300 lines processed...\n",
      "7400 lines processed...\n",
      "7500 lines processed...\n",
      "7600 lines processed...\n",
      "7700 lines processed...\n",
      "7800 lines processed...\n",
      "7900 lines processed...\n",
      "8000 lines processed...\n",
      "8100 lines processed...\n",
      "8200 lines processed...\n",
      "8300 lines processed...\n",
      "8400 lines processed...\n",
      "8500 lines processed...\n",
      "8600 lines processed...\n",
      "8700 lines processed...\n",
      "8800 lines processed...\n",
      "8900 lines processed...\n",
      "9000 lines processed...\n",
      "9100 lines processed...\n",
      "9200 lines processed...\n",
      "9300 lines processed...\n",
      "9400 lines processed...\n",
      "9500 lines processed...\n",
      "9600 lines processed...\n",
      "9700 lines processed...\n",
      "9800 lines processed...\n",
      "9900 lines processed...\n",
      "10000 lines processed...\n",
      "10100 lines processed...\n",
      "10200 lines processed...\n",
      "10300 lines processed...\n",
      "10400 lines processed...\n",
      "10500 lines processed...\n",
      "10600 lines processed...\n",
      "10700 lines processed...\n",
      "10800 lines processed...\n",
      "10900 lines processed...\n",
      "11000 lines processed...\n",
      "11100 lines processed...\n",
      "11200 lines processed...\n",
      "11300 lines processed...\n",
      "11400 lines processed...\n",
      "11500 lines processed...\n",
      "11600 lines processed...\n",
      "11700 lines processed...\n",
      "11800 lines processed...\n",
      "11900 lines processed...\n",
      "12000 lines processed...\n",
      "12100 lines processed...\n",
      "12200 lines processed...\n",
      "12300 lines processed...\n",
      "12400 lines processed...\n",
      "CPU times: user 1h 31min 45s, sys: 2min 52s, total: 1h 34min 37s\n",
      "Wall time: 47min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "infile          = 'ROClaws20180424C.txt'\n",
    "# segmented words, no POS tags\n",
    "outfile_seg     = 'ROClaws20180424C.seg.txt'\n",
    "# POS tags only, no words\n",
    "outfile_pos     = 'ROClaws20180424C.pos.txt'\n",
    "# segmented words + POS tags\n",
    "outfile_seg_pos = 'ROClaws20180424C.seg.pos.txt'\n",
    "\n",
    "delim = ' '\n",
    "BATCH_SIZE = 100\n",
    "cnt = 0\n",
    "batch = 0\n",
    "\n",
    "with open(outfile_seg, \"w\", encoding='utf8', newline='\\n') as fo1:\n",
    "    with open(outfile_pos, \"w\", encoding='utf8', newline='\\n') as fo2:\n",
    "        with open(infile, 'r', encoding='utf8') as fi:\n",
    "            sentences = []\n",
    "            for line in fi:\n",
    "                cnt += 1\n",
    "                batch += 1\n",
    "                sentences.append(line.strip())\n",
    "                if batch == BATCH_SIZE:\n",
    "                    results1 = ws(sentences)\n",
    "                    for sent in results1:\n",
    "                        fo1.write(delim.join(sent)+\"\\n\")\n",
    "                    results2 = pos(results1)\n",
    "                    for sent in results2:\n",
    "                        fo2.write(delim.join(sent)+\"\\n\")\n",
    "                    batch = 0 # reset counter\n",
    "                    sentences = []\n",
    "                if cnt % (BATCH_SIZE * 1) == 0:\n",
    "                    print(f\"{cnt} lines processed...\")\n",
    "                    fo1.flush()\n",
    "                    fo2.flush()\n",
    "            if len(sentences) > 0:\n",
    "                results1 = ws(sentences)\n",
    "                for sent in results1:\n",
    "                    fo1.write(delim.join(sent)+\"\\n\")\n",
    "                results2 = pos(results1)\n",
    "                for sent in results2:\n",
    "                    fo2.write(delim.join(sent)+\"\\n\")   \n",
    "\n",
    "delim2 = '_'\n",
    "with open(outfile_seg_pos, \"w\", encoding='utf8', newline='\\n') as fo3:\n",
    "    with open(outfile_seg, \"r\", encoding='utf8') as fi1:\n",
    "        with open(outfile_pos, \"r\", encoding='utf8') as fi2:\n",
    "            for line1 in fi1:\n",
    "                line2 = next(fi2)\n",
    "                pairs = zip(line1.strip().split(delim), line2.strip().split(delim))\n",
    "                fo3.write(delim.join([ f\"{p[0]}{delim2}{p[1]}\" for p in pairs]) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oW2PziLcwrTY"
   },
   "source": [
    "Finally, make sure all the files have the same line count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZZVUNoInlJV2"
   },
   "outputs": [],
   "source": [
    "!wc -l {infile} {outfile_seg} {outfile_pos} {outfile_seg_pos}"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ckipnn_colab_gpu.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
