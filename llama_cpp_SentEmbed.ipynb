{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJagHNg4tIJ_",
        "outputId": "667ca81a-484b-42aa-8b8d-a287d4d7739c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Feb 11 05:43:00 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   59C    P8             12W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "!pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu124"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CH7BuC1VtTxw",
        "outputId": "49403ca3-66b0-4652-cb08-6fdfcd5c3f92"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://abetlen.github.io/llama-cpp-python/whl/cu124\n",
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.3.7.tar.gz (66.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.7/66.7 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (1.26.4)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (3.1.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.7-cp311-cp311-linux_x86_64.whl size=4601142 sha256=2c288148e0fb1f4885f5d33db6806ed5d725909328b200913f04c90a08109261\n",
            "  Stored in directory: /root/.cache/pip/wheels/eb/82/79/ac77fcd49324b75ae6aa18e63a87cf9da4371a57e2cdc8dc03\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.3.7\n",
            "CPU times: user 820 ms, sys: 117 ms, total: 937 ms\n",
            "Wall time: 2min 36s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama"
      ],
      "metadata": {
        "id": "5V3vI9pAueto"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/ChristianAzinn/labse-gguf/resolve/main/labse.Q8_0.gguf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDV9co1yvaeD",
        "outputId": "d42a0ec8-caf7-4afc-fceb-e9d6556dfc0b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-11 05:46:49--  https://huggingface.co/ChristianAzinn/labse-gguf/resolve/main/labse.Q8_0.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 13.35.202.34, 13.35.202.97, 13.35.202.40, ...\n",
            "Connecting to huggingface.co (huggingface.co)|13.35.202.34|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.hf.co/repos/16/de/16de140bf5e3123a1aea5c5d6cde9199378efa2f30553724d633c8cd3228a51f/89ec00269f2d30015194e49f0b6b1a2b9affe6d14c46762c4a0edd425f41a171?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27labse.Q8_0.gguf%3B+filename%3D%22labse.Q8_0.gguf%22%3B&Expires=1739256409&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczOTI1NjQwOX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzE2L2RlLzE2ZGUxNDBiZjVlMzEyM2ExYWVhNWM1ZDZjZGU5MTk5Mzc4ZWZhMmYzMDU1MzcyNGQ2MzNjOGNkMzIyOGE1MWYvODllYzAwMjY5ZjJkMzAwMTUxOTRlNDlmMGI2YjFhMmI5YWZmZTZkMTRjNDY3NjJjNGEwZWRkNDI1ZjQxYTE3MT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=d0uxvG00SN3f%7EeaICnyCt0vnFNg6JTgmw53opsu11jg7khphR7%7EKP0JVWw0CVk3eAssYh8xQaHanC2paAvKpXVG1bIe9g200trdT9oMgb9Q7J7Zkw38qGY0JR1zBZbsfIzMf9BQ76r8-FFNfjbwD6JsmqsG1TSYhVUMcUUClv1ETYY49rt%7E8XdWVW0Ds7dIB-WGkwlLnIgoH%7Ed-ymHDShrceigjQSFGKmDSpBZ3IFKvaRtOUCRLfHDVXJpEqMIN8XwJdpxxcy%7EVILNCY4vk6irHrBT%7EDeXBK%7ESlvsq%7EOaeedNe8kkKcR%7ENnHrvi8hQWSY6G3QaSQwbmaREBeYP4A6g__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
            "--2025-02-11 05:46:49--  https://cdn-lfs-us-1.hf.co/repos/16/de/16de140bf5e3123a1aea5c5d6cde9199378efa2f30553724d633c8cd3228a51f/89ec00269f2d30015194e49f0b6b1a2b9affe6d14c46762c4a0edd425f41a171?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27labse.Q8_0.gguf%3B+filename%3D%22labse.Q8_0.gguf%22%3B&Expires=1739256409&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczOTI1NjQwOX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzE2L2RlLzE2ZGUxNDBiZjVlMzEyM2ExYWVhNWM1ZDZjZGU5MTk5Mzc4ZWZhMmYzMDU1MzcyNGQ2MzNjOGNkMzIyOGE1MWYvODllYzAwMjY5ZjJkMzAwMTUxOTRlNDlmMGI2YjFhMmI5YWZmZTZkMTRjNDY3NjJjNGEwZWRkNDI1ZjQxYTE3MT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=d0uxvG00SN3f%7EeaICnyCt0vnFNg6JTgmw53opsu11jg7khphR7%7EKP0JVWw0CVk3eAssYh8xQaHanC2paAvKpXVG1bIe9g200trdT9oMgb9Q7J7Zkw38qGY0JR1zBZbsfIzMf9BQ76r8-FFNfjbwD6JsmqsG1TSYhVUMcUUClv1ETYY49rt%7E8XdWVW0Ds7dIB-WGkwlLnIgoH%7Ed-ymHDShrceigjQSFGKmDSpBZ3IFKvaRtOUCRLfHDVXJpEqMIN8XwJdpxxcy%7EVILNCY4vk6irHrBT%7EDeXBK%7ESlvsq%7EOaeedNe8kkKcR%7ENnHrvi8hQWSY6G3QaSQwbmaREBeYP4A6g__&Key-Pair-Id=K24J24Z295AEI9\n",
            "Resolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 3.171.198.22, 3.171.198.59, 3.171.198.97, ...\n",
            "Connecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|3.171.198.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 514881920 (491M) [binary/octet-stream]\n",
            "Saving to: ‘labse.Q8_0.gguf’\n",
            "\n",
            "labse.Q8_0.gguf     100%[===================>] 491.03M  23.3MB/s    in 21s     \n",
            "\n",
            "2025-02-11 05:47:11 (23.0 MB/s) - ‘labse.Q8_0.gguf’ saved [514881920/514881920]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"./labse.Q8_0.gguf\"\n",
        "llm = Llama(model_path=model_path, embedding=True, n_gpu_layers=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEzWjUHvue5i",
        "outputId": "5e2e0135-0ae1-4790-9594-c74384eecd5f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ./labse.Q8_0.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = bert\n",
            "llama_model_loader: - kv   1:                               general.name str              = LaBSE\n",
            "llama_model_loader: - kv   2:                           bert.block_count u32              = 12\n",
            "llama_model_loader: - kv   3:                        bert.context_length u32              = 512\n",
            "llama_model_loader: - kv   4:                      bert.embedding_length u32              = 768\n",
            "llama_model_loader: - kv   5:                   bert.feed_forward_length u32              = 3072\n",
            "llama_model_loader: - kv   6:                  bert.attention.head_count u32              = 12\n",
            "llama_model_loader: - kv   7:          bert.attention.layer_norm_epsilon f32              = 0.000000\n",
            "llama_model_loader: - kv   8:                          general.file_type u32              = 7\n",
            "llama_model_loader: - kv   9:                      bert.attention.causal bool             = false\n",
            "llama_model_loader: - kv  10:                          bert.pooling_type u32              = 2\n",
            "llama_model_loader: - kv  11:            tokenizer.ggml.token_type_count u32              = 2\n",
            "llama_model_loader: - kv  12:                tokenizer.ggml.bos_token_id u32              = 101\n",
            "llama_model_loader: - kv  13:                tokenizer.ggml.eos_token_id u32              = 102\n",
            "llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,501153]  = [\"[PAD]\", \"[unused1]\", \"[unused2]\", \"...\n",
            "llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,501153]  = [-1000.000000, -1000.000000, -1000.00...\n",
            "llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,501153]  = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100\n",
            "llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102\n",
            "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  21:                tokenizer.ggml.cls_token_id u32              = 101\n",
            "llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103\n",
            "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:  124 tensors\n",
            "llama_model_loader: - type q8_0:   73 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q8_0\n",
            "print_info: file size   = 478.03 MiB (8.53 BPW) \n",
            "init_tokenizer: initializing tokenizer for type 3\n",
            "load: control token:    103 '[MASK]' is not marked as EOG\n",
            "load: control token:    101 '[CLS]' is not marked as EOG\n",
            "load: control token:    100 '[UNK]' is not marked as EOG\n",
            "load: control token:    102 '[SEP]' is not marked as EOG\n",
            "load: control token:      0 '[PAD]' is not marked as EOG\n",
            "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "load: special tokens cache size = 5\n",
            "load: token to piece cache size = 4.6241 MB\n",
            "print_info: arch             = bert\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 512\n",
            "print_info: n_embd           = 768\n",
            "print_info: n_layer          = 12\n",
            "print_info: n_head           = 12\n",
            "print_info: n_head_kv        = 12\n",
            "print_info: n_rot            = 64\n",
            "print_info: n_swa            = 0\n",
            "print_info: n_embd_head_k    = 64\n",
            "print_info: n_embd_head_v    = 64\n",
            "print_info: n_gqa            = 1\n",
            "print_info: n_embd_k_gqa     = 768\n",
            "print_info: n_embd_v_gqa     = 768\n",
            "print_info: f_norm_eps       = 1.0e-12\n",
            "print_info: f_norm_rms_eps   = 0.0e+00\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: n_ff             = 3072\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 0\n",
            "print_info: pooling type     = 2\n",
            "print_info: rope type        = 2\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 10000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 512\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: ssm_d_conv       = 0\n",
            "print_info: ssm_d_inner      = 0\n",
            "print_info: ssm_d_state      = 0\n",
            "print_info: ssm_dt_rank      = 0\n",
            "print_info: ssm_dt_b_c_rms   = 0\n",
            "print_info: model type       = 109M\n",
            "print_info: model params     = 470.34 M\n",
            "print_info: general.name     = LaBSE\n",
            "print_info: vocab type       = WPM\n",
            "print_info: n_vocab          = 501153\n",
            "print_info: n_merges         = 0\n",
            "print_info: BOS token        = 101 '[CLS]'\n",
            "print_info: EOS token        = 102 '[SEP]'\n",
            "print_info: UNK token        = 100 '[UNK]'\n",
            "print_info: SEP token        = 102 '[SEP]'\n",
            "print_info: PAD token        = 0 '[PAD]'\n",
            "print_info: MASK token       = 103 '[MASK]'\n",
            "print_info: LF token         = 0 '[PAD]'\n",
            "print_info: EOG token        = 102 '[SEP]'\n",
            "print_info: max token length = 51\n",
            "load_tensors: layer   0 assigned to device CPU\n",
            "load_tensors: layer   1 assigned to device CPU\n",
            "load_tensors: layer   2 assigned to device CPU\n",
            "load_tensors: layer   3 assigned to device CPU\n",
            "load_tensors: layer   4 assigned to device CPU\n",
            "load_tensors: layer   5 assigned to device CPU\n",
            "load_tensors: layer   6 assigned to device CPU\n",
            "load_tensors: layer   7 assigned to device CPU\n",
            "load_tensors: layer   8 assigned to device CPU\n",
            "load_tensors: layer   9 assigned to device CPU\n",
            "load_tensors: layer  10 assigned to device CPU\n",
            "load_tensors: layer  11 assigned to device CPU\n",
            "load_tensors: layer  12 assigned to device CPU\n",
            "load_tensors: tensor 'token_embd.weight' (q8_0) (and 196 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
            "load_tensors:   CPU_Mapped model buffer size =   478.03 MiB\n",
            "llama_init_from_model: n_seq_max     = 1\n",
            "llama_init_from_model: n_ctx         = 512\n",
            "llama_init_from_model: n_ctx_per_seq = 512\n",
            "llama_init_from_model: n_batch       = 512\n",
            "llama_init_from_model: n_ubatch      = 512\n",
            "llama_init_from_model: flash_attn    = 0\n",
            "llama_init_from_model: freq_base     = 10000.0\n",
            "llama_init_from_model: freq_scale    = 1\n",
            "llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1\n",
            "llama_kv_cache_init: layer 0: n_embd_k_gqa = 768, n_embd_v_gqa = 768\n",
            "llama_kv_cache_init: layer 1: n_embd_k_gqa = 768, n_embd_v_gqa = 768\n",
            "llama_kv_cache_init: layer 2: n_embd_k_gqa = 768, n_embd_v_gqa = 768\n",
            "llama_kv_cache_init: layer 3: n_embd_k_gqa = 768, n_embd_v_gqa = 768\n",
            "llama_kv_cache_init: layer 4: n_embd_k_gqa = 768, n_embd_v_gqa = 768\n",
            "llama_kv_cache_init: layer 5: n_embd_k_gqa = 768, n_embd_v_gqa = 768\n",
            "llama_kv_cache_init: layer 6: n_embd_k_gqa = 768, n_embd_v_gqa = 768\n",
            "llama_kv_cache_init: layer 7: n_embd_k_gqa = 768, n_embd_v_gqa = 768\n",
            "llama_kv_cache_init: layer 8: n_embd_k_gqa = 768, n_embd_v_gqa = 768\n",
            "llama_kv_cache_init: layer 9: n_embd_k_gqa = 768, n_embd_v_gqa = 768\n",
            "llama_kv_cache_init: layer 10: n_embd_k_gqa = 768, n_embd_v_gqa = 768\n",
            "llama_kv_cache_init: layer 11: n_embd_k_gqa = 768, n_embd_v_gqa = 768\n",
            "llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB\n",
            "llama_init_from_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB\n",
            "llama_init_from_model:        CPU  output buffer size =     0.00 MiB\n",
            "llama_init_from_model:        CPU compute buffer size =    19.01 MiB\n",
            "llama_init_from_model: graph nodes  = 429\n",
            "llama_init_from_model: graph splits = 1\n",
            "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | AVX512 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
            "Model metadata: {'tokenizer.ggml.mask_token_id': '103', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.seperator_token_id': '102', 'tokenizer.ggml.unknown_token_id': '100', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'bert', 'tokenizer.ggml.eos_token_id': '102', 'general.architecture': 'bert', 'bert.block_count': '12', 'bert.attention.layer_norm_epsilon': '0.000000', 'bert.context_length': '512', 'bert.feed_forward_length': '3072', 'bert.embedding_length': '768', 'tokenizer.ggml.cls_token_id': '101', 'tokenizer.ggml.token_type_count': '2', 'bert.attention.head_count': '12', 'tokenizer.ggml.bos_token_id': '101', 'general.file_type': '7', 'general.name': 'LaBSE', 'bert.attention.causal': 'false', 'bert.pooling_type': '2'}\n",
            "Using fallback chat format: llama-2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_embedding(text):\n",
        "    output = llm.embed(text)\n",
        "    return output\n",
        "\n",
        "def cosine_similarity(v1, v2):\n",
        "    v1_norm = np.linalg.norm(v1)\n",
        "    v2_norm = np.linalg.norm(v2)\n",
        "    if v1_norm == 0 or v2_norm == 0:\n",
        "        return 0.0  # Avoid division by zero\n",
        "    return np.dot(v1, v2) / (v1_norm * v2_norm)\n",
        "\n",
        "def cos_dist(v1, v2):\n",
        "    v1_norm = np.linalg.norm(v1)\n",
        "    v2_norm = np.linalg.norm(v2)\n",
        "    if v1_norm == 0 or v2_norm == 0:\n",
        "        return None  # Avoid division by zero\n",
        "    return 1 - (np.dot(v1, v2) / (v1_norm * v2_norm))\n",
        "\n"
      ],
      "metadata": {
        "id": "k1EVQa9kufIH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "sentence0 = \"今天天氣好熱。\"\n",
        "sentence1 = \"The weather sure is hot today.\"\n",
        "sentence2 = \"今天天气好热。\"\n",
        "\n",
        "embedding0 = get_embedding(sentence0)\n",
        "embedding1 = get_embedding(sentence1)\n",
        "embedding2 = get_embedding(sentence2)\n",
        "\n",
        "cos_dist01 = cos_dist(embedding0, embedding1)\n",
        "cos_dist02 = cos_dist(embedding0, embedding2)\n",
        "\n",
        "print(f\"Cosine distance between sentence 0 and 1: {cos_dist01}\")\n",
        "print(f\"osine distance between sentence 0 and 2: {cos_dist02}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CVKmiS1DLrq",
        "outputId": "bb27c825-8a08-494a-e09b-e31dfb237987"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =      51.28 ms\n",
            "llama_perf_context_print: prompt eval time =      44.49 ms /     9 tokens (    4.94 ms per token,   202.29 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      48.96 ms /    10 tokens\n",
            "llama_perf_context_print:        load time =      51.28 ms\n",
            "llama_perf_context_print: prompt eval time =      43.19 ms /     9 tokens (    4.80 ms per token,   208.38 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      44.98 ms /    10 tokens\n",
            "llama_perf_context_print:        load time =      51.28 ms\n",
            "llama_perf_context_print: prompt eval time =      42.27 ms /     9 tokens (    4.70 ms per token,   212.91 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      43.96 ms /    10 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine distance between sentence 0 and 1: 0.15513960511426406\n",
            "osine distance between sentence 0 and 2: 0.0027118204153381287\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WkOUT-pTDL3V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}